print("Initializing")
import sys
from argparse import ArgumentParser
import os
from csv import DictReader

import pandas as pd
from scipy import __version__ as scipy_version
print(f"python version: {sys.version}")
print(f"scipy version: {scipy_version}")

from warnings import filterwarnings, simplefilter as warnings_simplefilter
warnings_simplefilter(action='ignore', category=FutureWarning)
filterwarnings("ignore", message="The number of unique classes is greater than 50%")

from Tester import *

##### Parser Arguments #####
parser = ArgumentParser(description="Run all tests")
parser.add_argument("-o", "--output", type=str, help=f"Output file name prefix, default is the result_{tester_constants.DEFAULT_RESULTS_FILENAME_PREFIX}_<TIMESTAMP>", default=tester_constants.DEFAULT_RESULTS_FILENAME_EXTENDED_PREFIX)
parser.add_argument("-s", "--skip", action="store_true", help=f"skip exceptions, default is {tester_constants.SKIP_EXCEPTIONS}. If true will write the errors to errors file", default=tester_constants.SKIP_EXCEPTIONS)
parser.add_argument("-n", "--names", type=str, nargs="+", help="Specific datasets to run, default is all", default=())
parser.add_argument("-p", "--prefixes", type=str, nargs="+", help="prefixes to datasets to run, default is all. Relevant only if names argument is not provided", default=())
parser.add_argument("-w", "--repair_window", type=float, nargs="+", help="Repair window sizes, default is all", default=tester_constants.REPAIR_WINDOW_TEST_SIZES)
parser.add_argument("-d", "--drift_size", type=int, help=f"size of the drift, default is between {tester_constants.MIN_DRIFT_SIZE} and {tester_constants.MAX_DRIFT_SIZE}", default=-1)
parser.add_argument("-c", "--count", type=int, help="Number of tests to run, default is running all", default=-1)
parser.add_argument("-t", "--test", type=str, help="Test dataset to run if you want to run a specific test")

args = parser.parse_args()

print(f"Running tests with {len(tester_constants.diagnosers_output_names)} diagnosers: {tester_constants.diagnosers_output_names}")

if args.test:
    print(f"Running single test for {args.test}")
    single_test.sanity_run(file_name=args.test + ".csv", diagnosers_data=tester_constants.diagnosers_data)
    sys.exit(0)

specific_datasets_string = ""
specific_datasets, specific_prefixes = args.names, args.prefixes
if specific_datasets:
    specific_datasets = tuple(map(str.lower, specific_datasets))
    specific_prefixes = ()
    specific_datasets_string = '-'.join(specific_datasets)
elif specific_prefixes:
    specific_prefixes = tuple(map(str.lower, specific_prefixes))
    specific_datasets_string = "prefix_" + '-'.join(specific_datasets)
if specific_datasets_string:
    print(f"Running tests for {specific_datasets_string}")

datasets_count = args.count
if datasets_count > 0:
    print(f"Running tests for the first {datasets_count} datasets")

skip_exceptions = args.skip
if skip_exceptions:
    print("Skip exceptions is set to True. Exceptions will be written to errors file.")
else:
    print("Skip exceptions is set to False. Exceptions will be raised.")

repair_window_test_sizes, repair_windows_string = tester_constants.REPAIR_WINDOW_TEST_SIZES, ""
if args.repair_window != tester_constants.REPAIR_WINDOW_TEST_SIZES:
    repair_window_test_sizes = args.repair_window
    repair_windows_string = "-".join(map(str, repair_window_test_sizes))
print(f"Running tests with repair window sizes: {repair_windows_string}")

min_drift_size, max_drift_size, drift_size_string = tester_constants.MIN_DRIFT_SIZE, tester_constants.MAX_DRIFT_SIZE, ""
if args.drift_size > 0:
    drift_size_string = str(args.drift_size)
    min_drift_size = max_drift_size = args.drift_size
print(f"Running tests with drift size: {drift_size_string} ({min_drift_size} -> {max_drift_size})")


##### Create Results DataFrame #####
raw_results = pd.DataFrame(columns=tester_constants.RAW_RESULTS_COLUMN_NAMES)
errors = pd.DataFrame(columns=["name", "error"])


###### Run Tests #####
single_test_function = run_single_test if tester_constants.DRIFT_SYNTHESIZING_VERSION == 1 else run_single_test_v2

with open(tester_constants.DATASET_DESCRIPTION_FILE_PATH, "r") as descriptions_file:
    descriptions_reader = DictReader(descriptions_file)
    for dataset_description in descriptions_reader:
        if not datasets_count:
            break

        dataset_name = dataset_description["name"]
        
        if specific_datasets and dataset_name.lower() not in specific_datasets:
            continue
        if specific_prefixes and not dataset_name.lower().startswith(specific_prefixes):
            continue
        
        datasets_count -= 1

        print(f"Running tests for {dataset_name}")
        for test_result in single_test_function(tester_constants.DATASETS_DIRECTORY_FULL_PATH, dataset_name, repair_window_test_sizes=repair_window_test_sizes, min_drift_size=min_drift_size, max_drift_size=max_drift_size, diagnosers_data=tester_constants.diagnosers_data):
            if isinstance(test_result, Exception):
                if not skip_exceptions:
                    raise test_result
                errors = errors._append({"name": dataset_name, "error": test_result}, ignore_index=True)
                continue
            raw_results = raw_results._append(test_result, ignore_index=True)


##### Prepare Output Files #####
temp_files_suffix = ""
if drift_size_string:
    temp_files_suffix += f"_drift_size_{drift_size_string}"
if repair_windows_string:
    temp_files_suffix += f"_repair_window_{repair_windows_string}"
if specific_datasets_string:
    temp_files_suffix += f"_{specific_datasets_string}"

output_files_suffix, output_path = args.output, tester_constants.OUTPUT_DIRECTORY_FULL_PATH
if temp_files_suffix:
    output_path = tester_constants.TEMP_OUTPUT_DIRECTORY_FULL_PATH
    if not output_files_suffix.startswith(tester_constants.DEFAULT_RESULTS_FILENAME_PREFIX):
        output_path = f"{output_path}_{output_files_suffix}"
    output_files_suffix = temp_files_suffix
results_file_name, errors_file_name = f"{tester_constants.RESULTS_FILE_NAME_PREFIX}_{output_files_suffix}.csv", f"{tester_constants.ERRORS_FILE_NAME_PREFIX}_{output_files_suffix}.csv"
if raw_results.empty:
    results_file_name = f"{tester_constants.EMPTY_RESULTS_FILE_NAME_PREFIX}_{results_file_name}"

results_file_path = os.path.join(output_path, results_file_name)
errors_file_path = os.path.join(output_path, errors_file_name)


##### Save Results and Errors #####
os.makedirs(output_path, exist_ok=True)

raw_results.to_csv(results_file_path, index=False)

if not errors.empty:
    errors.to_csv(errors_file_path, index=False)
elif os.path.exists(errors_file_path):
    os.remove(errors_file_path)

print("All tests are done! Wasted-Effort; Correctly-Identified; Accuracy Increase:")
for baseline_output_name in tester_constants.BASELINES_OUTPUT_NAMES:
    print(f"{baseline_output_name}: XXXX; XXXX; {raw_results[f'{baseline_output_name} {tester_constants.FIX_ACCURACY_INCREASE_NAME_SUFFIX}'].mean():.2f}%")
print()
for diagnoser_output_name in tester_constants.diagnosers_output_names:
    diagnoser_diagnosis_results = f"{diagnoser_output_name}: XXXX; XXXX;"
    if diagnoser_output_name != Oracle.__name__:
        diagnoser_diagnosis_results = f"{diagnoser_output_name}: {raw_results[f'{diagnoser_output_name} {tester_constants.WASTED_EFFORT_NAME_SUFFIX}'].mean():.2f}; {raw_results[f'{diagnoser_output_name} {tester_constants.CORRECTLY_IDENTIFIED_NAME_SUFFIX}'].mean():.2f}%;"
    for fixer_output_name in tester_constants.fixers_output_names:
        print(f"{diagnoser_diagnosis_results} {fixer_output_name} {raw_results[f'{diagnoser_output_name}-{fixer_output_name} {tester_constants.FIX_ACCURACY_INCREASE_NAME_SUFFIX}'].mean():.2f}%")

print()
print(f"Results saved to {results_file_path}")